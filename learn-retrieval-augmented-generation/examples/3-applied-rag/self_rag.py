# -*- coding: utf-8 -*-
"""self rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EMo92hON1vji9hQsEg2zz3M-pBIRHCW5
"""

import pandas as pd
df = pd.read_csv('/content/sowjanya_resume_data.csv')
#df = df[df['variety'].notna()] # remove any NaN values as it blows up serialization
data = df # Get only 700 records. More records will make it slower to index
len(data)

!pip install qdrant_client

!pip install sentence_transformers

from qdrant_client import models, QdrantClient
from sentence_transformers import SentenceTransformer

encoder = SentenceTransformer('all-MiniLM-L6-v2')

# create the vector database client
qdrant = QdrantClient(":memory:") # Create in-memory Qdrant instance

# Create collection to store wines
qdrant.recreate_collection(
    collection_name="resume",
    vectors_config=models.VectorParams(
        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model
        distance=models.Distance.COSINE
    )
)

import csv
from qdrant_client import models, QdrantClient
from sentence_transformers import SentenceTransformer

encoder = SentenceTransformer('all-MiniLM-L6-v2')
qdrant = QdrantClient(":memory:") # Create in-memory Qdrant instance

# Create collection to store wines
qdrant.recreate_collection(
    collection_name="resume",
    vectors_config=models.VectorParams(
        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model
        distance=models.Distance.COSINE
    )
)

# Create the "resume" collection before uploading points
qdrant.recreate_collection(  # Create the "resume" collection
    collection_name="resume",
    vectors_config=models.VectorParams(
        size=encoder.get_sentence_embedding_dimension(),
        distance=models.Distance.COSINE
    )
)


# Load CSV data correctly into dictionaries
data = []
with open('sowjanya_resume_data.csv', newline='', encoding='utf-8') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        data.append(row)

# Print to check the structure of `data`
for idx, doc in enumerate(data):
    print(f"Row {idx}: {doc}")
    print(f"Keys in this row: {list(doc.keys())}")

# Ensure that you are encoding text columns like 'Summary'
qdrant.upload_points(
    collection_name="resume",
    points=[
        models.PointStruct(
            id=idx,
            vector=encoder.encode(doc.get("Summary", "")).tolist(),  # Ensure 'Summary' exists; default to empty string
            payload=doc,  # Attach the entire row as the payload
        ) for idx, doc in enumerate(data)
    ]
)

user_prompt = "about sowjanya"

# Search time for awesome wines!

hits = qdrant.search(
    collection_name="resume",
    query_vector=encoder.encode(user_prompt).tolist(),
    limit=3
)
for hit in hits:
  print(hit.payload, "score:", hit.score)

# define a variable to hold the search results
search_results = [hit.payload for hit in hits]

!pip install openai

# Now time to connect to the local large language model
from openai import OpenAI
client = OpenAI(
    base_url="http://127.0.0.1:8080/", # "http://<Your api-server IP>:port"
    api_key = "not-needed"
)
completion = client.chat.completions.create(
    model="llama.cpp",  # Change the model name to llama2-13b-chat for `llama.cpp` or whatever name you defined.
    messages=[
        {"role": "system", "content": "You are chatbot, a chatbot for sowjanya. Your top priority is to help answer users into selecting amazing wine and guide them with their requests."},
        {"role": "user", "content": "about sowjanya"},
        {"role": "assistant", "content": str(search_results)}
    ]
)
print(completion.choices[0].message)